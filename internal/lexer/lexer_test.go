package lexer

import (
	"strings"
	"testing"
)

// ==================== Token Type Tests ====================

func TestLexer_TokenWhitespace(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"space", " ", " "},
		{"tab", "\t", "\t"},
		{"newline", "\n", "\n"},
		{"crlf", "\r\n", "\r\n"},
		{"mixed", " \t\n  \t", " \t\n  \t"},
		{"multiple_newlines", "\n\n\n", "\n\n\n"},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens (whitespace + EOF), got %d", len(tokens))
			}
			if tokens[0].Type != TokenWhitespace {
				t.Errorf("expected TokenWhitespace, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenComment(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"single_line", "/* comment ;", "/* comment ;"},
		{"with_newline", "/* line1\nline2 ;", "/* line1\nline2 ;"},
		{"empty", "/* ;", "/* ;"},
		// SSL comments end at the FIRST semicolon
		{"with_semicolon_inside", "/* a;", "/* a;"},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenComment {
				t.Errorf("expected TokenComment, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenComment_Unclosed(t *testing.T) {
	// Comment without terminating semicolon should still be captured
	input := "/* unclosed comment"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	if len(tokens) < 2 {
		t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
	}
	if tokens[0].Type != TokenComment {
		t.Errorf("expected TokenComment for unclosed comment, got %s", tokens[0].Type)
	}
	if tokens[0].Text != input {
		t.Errorf("expected %q, got %q", input, tokens[0].Text)
	}
}

func TestLexer_TokenString(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"double_quote", `"hello"`, `"hello"`},
		{"single_quote", `'hello'`, `'hello'`},
		{"bracket_quote", `[hello]`, `[hello]`},
		{"empty_double", `""`, `""`},
		{"empty_single", `''`, `''`},
		{"with_spaces", `"hello world"`, `"hello world"`},
		{"with_newline", "\"hello\nworld\"", "\"hello\nworld\""},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenString {
				t.Errorf("expected TokenString, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenString_Escaped(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"escaped_quote", `"hello\"world"`, `"hello\"world"`},
		{"escaped_backslash", `"hello\\world"`, `"hello\\world"`},
		{"multiple_escapes", `"a\"b\"c"`, `"a\"b\"c"`},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenString {
				t.Errorf("expected TokenString, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenString_Unicode(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{"spanish", `"Ã‘oÃ±o"`},
		{"japanese", `"æ—¥æœ¬èªž"`},
		{"emoji", `"Hello ðŸŽ‰"`},
		{"mixed", `"CafÃ©, æ—¥æœ¬, ðŸŒ"`},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenString {
				t.Errorf("expected TokenString, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.input {
				t.Errorf("expected %q, got %q", tc.input, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenString_Unclosed(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{"unclosed_double", `"hello`},
		{"unclosed_single", `'hello`},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			// Should still get a string token even if unclosed
			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenString {
				t.Errorf("expected TokenString for unclosed string, got %s", tokens[0].Type)
			}
		})
	}
}

func TestLexer_TokenNumber(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"integer", "123", "123"},
		{"zero", "0", "0"},
		{"decimal", "123.456", "123.456"},
		{"leading_dot", ".5", ".5"},
		{"trailing_dot", "5.", "5."},
		{"scientific_positive", "1.5e+10", "1.5e+10"},
		{"scientific_negative", "2.3e-5", "2.3e-5"},
		{"scientific_no_sign", "1e10", "1e10"},
		{"uppercase_e", "1E10", "1E10"},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenNumber {
				t.Errorf("expected TokenNumber, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenKeyword(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"if", ":IF", ":IF"},
		{"endif", ":ENDIF", ":ENDIF"},
		{"while", ":WHILE", ":WHILE"},
		{"procedure", ":PROCEDURE", ":PROCEDURE"},
		{"declare", ":DECLARE", ":DECLARE"},
		{"public", ":PUBLIC", ":PUBLIC"},
		{"for", ":FOR", ":FOR"},
		{"try", ":TRY", ":TRY"},
		{"lowercase", ":if", ":if"},
		{"mixed_case", ":If", ":If"},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenKeyword {
				t.Errorf("expected TokenKeyword, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenKeyword_AllKeywords(t *testing.T) {
	// Test a selection of important keywords
	keywords := []string{
		":PROCEDURE", ":ENDPROC", ":IF", ":ELSE", ":ELSEIF", ":ENDIF",
		":WHILE", ":ENDWHILE", ":FOR", ":NEXT", ":TO", ":STEP",
		":TRY", ":CATCH", ":FINALLY", ":ENDTRY",
		":DECLARE", ":PUBLIC", ":PARAMETERS", ":DEFAULT",
		":BEGINCASE", ":CASE", ":OTHERWISE", ":ENDCASE", ":EXITCASE",
		":RETURN", ":EXIT", ":CLASS", ":INHERIT", ":REGION", ":ENDREGION",
	}

	for _, kw := range keywords {
		t.Run(kw, func(t *testing.T) {
			lex := NewLexer(kw)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenKeyword {
				t.Errorf("expected TokenKeyword for %s, got %s", kw, tokens[0].Type)
			}
		})
	}
}

func TestLexer_TokenIdentifier(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"simple", "myVar", "myVar"},
		{"with_underscore", "my_var", "my_var"},
		{"with_digits", "var123", "var123"},
		{"underscore_prefix", "_private", "_private"},
		{"all_caps", "CONSTANT", "CONSTANT"},
		{"mixed_case", "camelCase", "camelCase"},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenIdentifier {
				t.Errorf("expected TokenIdentifier, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenOperator(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
		typ   TokenType
	}{
		{"plus", "+", "+", TokenOperator},
		{"minus", "-", "-", TokenOperator},
		{"multiply", "*", "*", TokenOperator},
		{"divide", "/", "/", TokenOperator},
		{"power", "^", "^", TokenOperator},
		{"modulo", "%", "%", TokenOperator},
		{"equal", "=", "=", TokenOperator},
		{"less_than", "<", "<", TokenOperator},
		{"greater_than", ">", ">", TokenOperator},
		{"assignment", ":=", ":=", TokenOperator},
		{"hash", "#", "#", TokenOperator},
		{"dollar", "$", "$", TokenOperator},
		// Note: Some compound operators like !=, <=, >= may be tokenized
		// as separate tokens depending on constants.IsSSLCompoundOperator
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != tc.typ {
				t.Errorf("expected %s, got %s", tc.typ, tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenOperator_Logical(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"and", ".AND.", ".AND."},
		{"or", ".OR.", ".OR."},
		{"not", ".NOT.", ".NOT."},
		{"lowercase_and", ".and.", ".and."},
		{"lowercase_or", ".or.", ".or."},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenOperator {
				t.Errorf("expected TokenOperator, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

func TestLexer_TokenOperator_Boolean(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{"true", ".T."},
		{"false", ".F."},
		{"lowercase_true", ".t."},
		{"lowercase_false", ".f."},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			// Boolean literals are treated as keywords
			if tokens[0].Type != TokenKeyword {
				t.Errorf("expected TokenKeyword for boolean literal, got %s", tokens[0].Type)
			}
		})
	}
}

func TestLexer_TokenPunctuation(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"open_paren", "(", "("},
		{"close_paren", ")", ")"},
		{"open_brace", "{", "{"},
		{"close_brace", "}", "}"},
		{"semicolon", ";", ";"},
		{"comma", ",", ","},
		{"close_bracket", "]", "]"},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}
			if tokens[0].Type != TokenPunctuation {
				t.Errorf("expected TokenPunctuation, got %s", tokens[0].Type)
			}
			if tokens[0].Text != tc.want {
				t.Errorf("expected %q, got %q", tc.want, tokens[0].Text)
			}
		})
	}
}

// ==================== Context-Sensitive Tests ====================

func TestLexer_ArrayAccess(t *testing.T) {
	tests := []struct {
		name       string
		input      string
		wantBefore TokenType
		wantText   string
	}{
		{"after_identifier", "arr[0]", TokenPunctuation, "["},
		{"after_paren", "func()[0]", TokenPunctuation, "["},
		{"after_bracket", "arr[0][1]", TokenPunctuation, "["},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			// Find the [ token
			var bracketToken *Token
			for i := range tokens {
				if tokens[i].Text == "[" {
					bracketToken = &tokens[i]
					break
				}
			}

			if bracketToken == nil {
				t.Fatal("expected to find [ token")
			}
			if bracketToken.Type != tc.wantBefore {
				t.Errorf("expected %s for [, got %s", tc.wantBefore, bracketToken.Type)
			}
		})
	}
}

func TestLexer_BracketString_AtLineStart(t *testing.T) {
	// [ at line start or after operators should be a string delimiter
	input := "[hello]"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	if len(tokens) < 2 {
		t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
	}
	if tokens[0].Type != TokenString {
		t.Errorf("expected TokenString for bracket string, got %s", tokens[0].Type)
	}
	if tokens[0].Text != "[hello]" {
		t.Errorf("expected [hello], got %q", tokens[0].Text)
	}
}

func TestLexer_ColonContext(t *testing.T) {
	tests := []struct {
		name     string
		input    string
		wantType TokenType
		wantText string
	}{
		{"assignment", "x := 1", TokenOperator, ":="},
		{"keyword_if", ":IF", TokenKeyword, ":IF"},
		{"keyword_procedure", ":PROCEDURE", TokenKeyword, ":PROCEDURE"},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			lex := NewLexer(tc.input)
			tokens := lex.Tokenize()

			// Find the token we're looking for
			found := false
			for _, tok := range tokens {
				if tok.Text == tc.wantText {
					found = true
					if tok.Type != tc.wantType {
						t.Errorf("expected %s for %q, got %s", tc.wantType, tc.wantText, tok.Type)
					}
					break
				}
			}
			if !found {
				t.Errorf("expected to find token %q", tc.wantText)
			}
		})
	}
}

func TestLexer_ColonAfterIdentifier(t *testing.T) {
	// x:y should tokenize as identifier, punctuation, identifier (not keyword)
	input := "obj:method"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	// Filter out EOF
	var nonEOF []Token
	for _, t := range tokens {
		if t.Type != TokenEOF {
			nonEOF = append(nonEOF, t)
		}
	}

	if len(nonEOF) != 3 {
		t.Fatalf("expected 3 tokens, got %d: %v", len(nonEOF), nonEOF)
	}
	if nonEOF[0].Type != TokenIdentifier || nonEOF[0].Text != "obj" {
		t.Errorf("expected identifier 'obj', got %s %q", nonEOF[0].Type, nonEOF[0].Text)
	}
	if nonEOF[1].Type != TokenPunctuation || nonEOF[1].Text != ":" {
		t.Errorf("expected punctuation ':', got %s %q", nonEOF[1].Type, nonEOF[1].Text)
	}
	if nonEOF[2].Type != TokenIdentifier || nonEOF[2].Text != "method" {
		t.Errorf("expected identifier 'method', got %s %q", nonEOF[2].Type, nonEOF[2].Text)
	}
}

// ==================== Position Tracking Tests ====================

func TestLexer_Position_LineColumn(t *testing.T) {
	input := "a\nb\nc"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	expected := []struct {
		text   string
		line   int
		column int
	}{
		{"a", 1, 1},
		{"\n", 1, 2},
		{"b", 2, 1},
		{"\n", 2, 2},
		{"c", 3, 1},
	}

	for i, exp := range expected {
		if i >= len(tokens) {
			t.Fatalf("not enough tokens")
		}
		tok := tokens[i]
		if tok.Text != exp.text {
			t.Errorf("token %d: expected text %q, got %q", i, exp.text, tok.Text)
		}
		if tok.Line != exp.line {
			t.Errorf("token %d (%q): expected line %d, got %d", i, tok.Text, exp.line, tok.Line)
		}
		if tok.Column != exp.column {
			t.Errorf("token %d (%q): expected column %d, got %d", i, tok.Text, exp.column, tok.Column)
		}
	}
}

func TestLexer_Position_AfterMultiCharToken(t *testing.T) {
	input := "abc def"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	// abc at column 1, space at column 4, def at column 5
	if tokens[0].Column != 1 {
		t.Errorf("expected 'abc' at column 1, got %d", tokens[0].Column)
	}
	if tokens[1].Column != 4 {
		t.Errorf("expected space at column 4, got %d", tokens[1].Column)
	}
	if tokens[2].Column != 5 {
		t.Errorf("expected 'def' at column 5, got %d", tokens[2].Column)
	}
}

func TestLexer_Position_Offset(t *testing.T) {
	input := "ab cd"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	if tokens[0].Offset != 0 {
		t.Errorf("expected 'ab' at offset 0, got %d", tokens[0].Offset)
	}
	if tokens[1].Offset != 2 {
		t.Errorf("expected space at offset 2, got %d", tokens[1].Offset)
	}
	if tokens[2].Offset != 3 {
		t.Errorf("expected 'cd' at offset 3, got %d", tokens[2].Offset)
	}
}

// ==================== Edge Case Tests ====================

func TestLexer_EdgeCase_EmptyInput(t *testing.T) {
	lex := NewLexer("")
	tokens := lex.Tokenize()

	if len(tokens) != 1 {
		t.Fatalf("expected 1 token (EOF), got %d", len(tokens))
	}
	if tokens[0].Type != TokenEOF {
		t.Errorf("expected TokenEOF, got %s", tokens[0].Type)
	}
}

func TestLexer_EdgeCase_OnlyWhitespace(t *testing.T) {
	lex := NewLexer("   \n\t  ")
	tokens := lex.Tokenize()

	if len(tokens) != 2 {
		t.Fatalf("expected 2 tokens (whitespace + EOF), got %d", len(tokens))
	}
	if tokens[0].Type != TokenWhitespace {
		t.Errorf("expected TokenWhitespace, got %s", tokens[0].Type)
	}
	if tokens[1].Type != TokenEOF {
		t.Errorf("expected TokenEOF, got %s", tokens[1].Type)
	}
}

func TestLexer_EdgeCase_VeryLongLine(t *testing.T) {
	// Create a very long identifier
	longIdent := strings.Repeat("a", 10000)
	lex := NewLexer(longIdent)
	tokens := lex.Tokenize()

	if len(tokens) != 2 {
		t.Fatalf("expected 2 tokens, got %d", len(tokens))
	}
	if tokens[0].Type != TokenIdentifier {
		t.Errorf("expected TokenIdentifier, got %s", tokens[0].Type)
	}
	if len(tokens[0].Text) != 10000 {
		t.Errorf("expected 10000 char identifier, got %d", len(tokens[0].Text))
	}
}

func TestLexer_EdgeCase_ManyTokens(t *testing.T) {
	// Create input with many tokens
	var parts []string
	for i := 0; i < 1000; i++ {
		parts = append(parts, "a")
	}
	input := strings.Join(parts, " ")
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	// Should have 1000 identifiers + 999 whitespace + 1 EOF = 2000 tokens
	expectedTokens := 1000 + 999 + 1
	if len(tokens) != expectedTokens {
		t.Errorf("expected %d tokens, got %d", expectedTokens, len(tokens))
	}
}

func TestLexer_EdgeCase_OnlyComments(t *testing.T) {
	input := "/* comment 1 ; /* comment 2 ;"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	// Should have 2 comments + 1 EOF
	commentCount := 0
	for _, tok := range tokens {
		if tok.Type == TokenComment {
			commentCount++
		}
	}
	if commentCount != 2 {
		t.Errorf("expected 2 comments, got %d", commentCount)
	}
}

// ==================== Helper Function Tests ====================

func TestGetTokenAtPosition(t *testing.T) {
	input := "abc def ghi"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	// Token positions: abc at col 1-3, space at col 4, def at col 5-7, space at col 8, ghi at col 9-11
	// GetTokenAtPosition uses <= for tokenEnd, so boundary columns match the previous token
	// We use columns that are clearly inside each token (not at boundaries)
	tests := []struct {
		name   string
		line   int
		column int
		want   string
	}{
		{"start_of_abc", 1, 1, "abc"},
		{"middle_of_abc", 1, 2, "abc"},
		{"inside_def", 1, 6, "def"},  // column 6 is clearly inside "def"
		{"inside_ghi", 1, 10, "ghi"}, // column 10 is clearly inside "ghi"
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			tok := GetTokenAtPosition(tokens, tc.line, tc.column)
			if tok == nil {
				t.Fatalf("expected token at line %d col %d, got nil", tc.line, tc.column)
			}
			if tok.Text != tc.want {
				t.Errorf("expected %q at line %d col %d, got %q", tc.want, tc.line, tc.column, tok.Text)
			}
		})
	}
}

func TestGetTokenAtPosition_NoMatch(t *testing.T) {
	input := "abc"
	lex := NewLexer(input)
	tokens := lex.Tokenize()

	tok := GetTokenAtPosition(tokens, 2, 1) // Line 2 doesn't exist
	if tok != nil {
		t.Errorf("expected nil for non-existent line, got %v", tok)
	}
}

func TestGetWordAtPosition(t *testing.T) {
	input := "hello world test"

	tests := []struct {
		line   int
		column int
		want   string
	}{
		{1, 1, "hello"},
		{1, 3, "hello"},
		{1, 7, "world"},
		{1, 13, "test"},
	}

	for _, tc := range tests {
		t.Run(tc.want, func(t *testing.T) {
			word := GetWordAtPosition(input, tc.line, tc.column)
			if word != tc.want {
				t.Errorf("expected %q at line %d col %d, got %q", tc.want, tc.line, tc.column, word)
			}
		})
	}
}

func TestGetWordAtPosition_AtBoundary(t *testing.T) {
	input := "hello world"

	// GetWordAtPosition finds word boundaries - column 6 is after "hello" ends
	// The function looks for word chars in both directions from the position
	// Column 6 is the space, which is not a word char, so it returns ""
	// But column 5 is still part of "hello", column 7 is start of "world"
	word := GetWordAtPosition(input, 1, 5)
	if word != "hello" {
		t.Errorf("expected 'hello' at column 5, got %q", word)
	}

	word = GetWordAtPosition(input, 1, 7)
	if word != "world" {
		t.Errorf("expected 'world' at column 7, got %q", word)
	}
}

func TestGetWordAtPosition_InvalidPosition(t *testing.T) {
	input := "hello"

	// Invalid line
	word := GetWordAtPosition(input, 0, 1)
	if word != "" {
		t.Errorf("expected empty string for invalid line, got %q", word)
	}

	// Invalid column
	word = GetWordAtPosition(input, 1, 0)
	if word != "" {
		t.Errorf("expected empty string for invalid column, got %q", word)
	}
}

func TestGetWordAtPosition_MultiLine(t *testing.T) {
	input := "line1\nline2\nline3"

	tests := []struct {
		line int
		col  int
		want string
	}{
		{1, 1, "line1"},
		{2, 1, "line2"},
		{3, 1, "line3"},
	}

	for _, tc := range tests {
		word := GetWordAtPosition(input, tc.line, tc.col)
		if word != tc.want {
			t.Errorf("expected %q at line %d, got %q", tc.want, tc.line, word)
		}
	}
}

// ==================== Complete Statement Tests ====================

func TestLexer_CompleteStatement(t *testing.T) {
	input := `:PROCEDURE Test;
:DECLARE x, y;
x := 1 + 2;
:IF x > 0;
	y := x * 2;
:ENDIF;
:ENDPROC;`

	lex := NewLexer(input)
	tokens := lex.Tokenize()

	// Verify we got the expected token types in sequence
	// (simplified check - just verify we have a mix of token types)
	tokenTypes := make(map[TokenType]int)
	for _, tok := range tokens {
		tokenTypes[tok.Type]++
	}

	if tokenTypes[TokenKeyword] == 0 {
		t.Error("expected some keywords")
	}
	if tokenTypes[TokenIdentifier] == 0 {
		t.Error("expected some identifiers")
	}
	if tokenTypes[TokenOperator] == 0 {
		t.Error("expected some operators")
	}
	if tokenTypes[TokenNumber] == 0 {
		t.Error("expected some numbers")
	}
	if tokenTypes[TokenPunctuation] == 0 {
		t.Error("expected some punctuation")
	}
	if tokenTypes[TokenWhitespace] == 0 {
		t.Error("expected some whitespace")
	}
	if tokenTypes[TokenEOF] != 1 {
		t.Error("expected exactly one EOF")
	}
}

func TestLexer_SQLString(t *testing.T) {
	input := `ds := SQLExecute("SELECT * FROM users WHERE id = ?userId?");`

	lex := NewLexer(input)
	tokens := lex.Tokenize()

	// Find the string token
	var stringToken *Token
	for i := range tokens {
		if tokens[i].Type == TokenString {
			stringToken = &tokens[i]
			break
		}
	}

	if stringToken == nil {
		t.Fatal("expected to find a string token")
	}
	if !strings.Contains(stringToken.Text, "SELECT") {
		t.Errorf("expected SQL string to contain SELECT, got %q", stringToken.Text)
	}
}

// ==================== TokenType String Tests ====================

func TestTokenType_String(t *testing.T) {
	tests := []struct {
		typ  TokenType
		want string
	}{
		{TokenWhitespace, "Whitespace"},
		{TokenComment, "Comment"},
		{TokenString, "String"},
		{TokenNumber, "Number"},
		{TokenKeyword, "Keyword"},
		{TokenIdentifier, "Identifier"},
		{TokenOperator, "Operator"},
		{TokenPunctuation, "Punctuation"},
		{TokenUnknown, "Unknown"},
		{TokenEOF, "EOF"},
	}

	for _, tc := range tests {
		t.Run(tc.want, func(t *testing.T) {
			got := tc.typ.String()
			if got != tc.want {
				t.Errorf("expected %q, got %q", tc.want, got)
			}
		})
	}
}

// ==================== Benchmark Tests ====================

// generateSSLDocument creates a test SSL document with the specified number of procedures
func generateSSLDocument(procCount int) string {
	var sb strings.Builder
	for i := 0; i < procCount; i++ {
		sb.WriteString(`:PROCEDURE Proc`)
		sb.WriteString(string(rune('0' + i%10)))
		sb.WriteString(`;
:PARAMETERS param1, param2;
:DECLARE localVar, result;
:IF param1 > 0;
	result := param1 * param2;
:ELSE;
	result := 0;
:ENDIF;
:RETURN result;
:ENDPROC;

`)
	}
	return sb.String()
}

func BenchmarkLexer_Tokenize_Small(b *testing.B) {
	text := generateSSLDocument(10)
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		lex := NewLexer(text)
		_ = lex.Tokenize()
	}
}

func BenchmarkLexer_Tokenize_Medium(b *testing.B) {
	text := generateSSLDocument(100)
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		lex := NewLexer(text)
		_ = lex.Tokenize()
	}
}

func BenchmarkLexer_Tokenize_Large(b *testing.B) {
	text := generateSSLDocument(1000)
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		lex := NewLexer(text)
		_ = lex.Tokenize()
	}
}
